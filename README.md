# Web-Scraping
The repository contains Python Notebook of a Web Scraper that fetches information such as State, Region, Mayor, Counties etc. about the top cities in the United States. The repository also contains the final resulting CSV file. The technique is employed using many in-built Python libraries, of which BeautifulSoup is of significant importance. Large amounts of data is extracted from several wikipedia pages, cleaned, structured and is finally saved to a csv file with relevant fields in a format ready to be uploaded to a BigQuery table.
Starting from the webpage about the top Cities of USA, each city is linked with a URL to redirect to a webpage of its own. User defined functions fetch the Cities names, the urls linked to them, and the information from the redirected pages. Every WIkipedia page of a City contains a box on the right with the most relevant information about the city with the tag 'Infobox Geography VCard', as found while inspecting the HTML page. Using this box as a provider for details about the city, the information is added to the final layout of the Cities table without losing important and relevant data. This is followed by writing the scraped information in a CSV file ready to be used by BigQuery application for further analysis.
